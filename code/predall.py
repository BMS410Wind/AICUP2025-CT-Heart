import os
import torch
import glob
import numpy as np
from monai.networks.nets import DynUNet
from monai.transforms import (
    Compose, LoadImage, EnsureChannelFirst, Orientation,
    Spacing, ScaleIntensityRangePercentiles, CropForeground, SpatialPad
)
from monai.inferers import sliding_window_inference
import nibabel as nib
from tqdm import tqdm
import logging
from scipy.ndimage import label, generate_binary_structure, binary_fill_holes
from scipy.ndimage import binary_closing, binary_opening

# ==================== é…ç½® ====================
CONFIG = {
    # æ¨¡å‹è·¯å¾‘
    "model_path": r"D:\aicup\experiments\nnunet_all_classes\best_model_nnunet.pth",
    
    # æ¸¬è©¦è³‡æ–™
    "test_dir": r"D:\aicup\test\41_testing_image",
    "output_dir": r"D:\aicup\predictions\nnunet_all_classes",
    
    # æ¨¡å‹é…ç½®(éœ€èˆ‡è¨“ç·´æ™‚ä¸€è‡´)
    "in_channels": 1,
    "out_channels": 4,  # 0=èƒŒæ™¯, 1=å¿ƒè‡Ÿ, 2=å¿ƒåŒ…è†œ, 3=éˆ£åŒ–
    "spatial_dims": 3,
    "kernels": [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]],
    "strides": [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 1]],
    "deep_supervision": False,  # èˆ‡ trainall.py ä¸€è‡´
    "deep_supr_num": 2,
    
    # æ¨ç†è¨­å®š
    "target_spacing": (1.5, 1.5, 2.0),
    "roi_size": (128, 128, 128),
    "overlap": 0.6,  # èˆ‡è¨“ç·´æ™‚ sliding window ä¸€è‡´
    "sw_batch_size": 4,
    
    # å¾Œè™•ç†è¨­å®š
    "use_postprocess": True,
    "postprocess_mode": "aggressive",  # "minimal" / "moderate" / "aggressive"
    "min_size_class1": 500,   # å¿ƒè‡Ÿæœ€å°é«”ç´ æ•¸
    "min_size_class2": 5,    # å¿ƒåŒ…è†œæœ€å°é«”ç´ æ•¸
    "min_size_class3": 0,    # éˆ£åŒ–æœ€å°é«”ç´ æ•¸
    "fill_holes": True,
    "use_morphology": True,
    
    "device": "cuda" if torch.cuda.is_available() else "cpu"
}

os.makedirs(CONFIG["output_dir"], exist_ok=True)

# ==================== æ—¥èªŒè¨­å®š ====================
log_file = os.path.join(CONFIG["output_dir"], 'prediction.log')
file_handler = logging.FileHandler(log_file, encoding='utf-8')
file_handler.setLevel(logging.INFO)
stream_handler = logging.StreamHandler()
stream_handler.setLevel(logging.INFO)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[file_handler, stream_handler]
)
logger = logging.getLogger(__name__)

# ==================== å¾Œè™•ç†å‡½æ•¸ ====================
def remove_small_components(prediction, min_sizes):
    """ç§»é™¤å°ç¢ç‰‡"""
    result = prediction.copy()
    structure = generate_binary_structure(3, 3)
    
    for class_id, min_size in min_sizes.items():
        mask = (prediction == class_id)
        if np.any(mask):
            labeled, num_features = label(mask, structure=structure)
            component_sizes = np.bincount(labeled.ravel())
            
            for i in range(1, num_features + 1):
                if component_sizes[i] < min_size:
                    result[labeled == i] = 0
    
    return result

def fill_holes_all_classes(prediction):
    """å¡«è£œæ‰€æœ‰é¡åˆ¥çš„å…§éƒ¨å­”æ´"""
    result = prediction.copy()
    
    for class_id in [1, 2, 3]:
        mask = (prediction == class_id)
        if np.any(mask):
            filled = binary_fill_holes(mask)
            result[filled & ~mask] = class_id
    
    return result

# def morphological_smooth(prediction):
#     """å½¢æ…‹å­¸å¹³æ»‘"""
#     result = prediction.copy()
#     structure = generate_binary_structure(3, 1)
    
#     for class_id in [1, 2, 3]:
#         mask = (prediction == class_id)
#         if np.any(mask):
#             # é–‰é‹ç®—:é€£æ¥å°é–“éš™
#             mask = binary_closing(mask, structure=structure, iterations=1)
#             # é–‹é‹ç®—:ç§»é™¤å°çªèµ·
#             mask = binary_opening(mask, structure=structure, iterations=1)
            
#             result[mask] = class_id
#             result[~mask & (result == class_id)] = 0
    
#     return result
def morphological_smooth(prediction):
    """å½¢æ…‹å­¸å¹³æ»‘"""
    result = prediction.copy()
    
    for class_id in [1, 2, 3]:
        mask = (prediction == class_id)
        if np.any(mask):
            if class_id == 2:  # å¿ƒåŒ…è†œéœ€è¦æ›´å¤šè¿­ä»£
                # ä½¿ç”¨æ›´å¤§çš„çµæ§‹å…ƒç´ 
                structure = generate_binary_structure(3, 2)
                # é–‰é‹ç®—:é€£æ¥å°é–“éš™
                mask = binary_closing(mask, structure=structure, iterations=2)
                # é–‹é‹ç®—:ç§»é™¤å°çªèµ·
                mask = binary_opening(mask, structure=structure, iterations=1)
            else:
                structure = generate_binary_structure(3, 1)
                mask = binary_closing(mask, structure=structure, iterations=1)
                mask = binary_opening(mask, structure=structure, iterations=1)
            
            result[mask] = class_id
            result[~mask & (result == class_id)] = 0
    
    return result
def keep_largest_component_per_class(prediction, apply_to_classes):
    """ä¿ç•™æ¯å€‹é¡åˆ¥çš„æœ€å¤§é€£é€šåŸŸ"""
    result = prediction.copy()
    structure = generate_binary_structure(3, 3)
    
    for class_id in apply_to_classes:
        mask = (prediction == class_id)
        if np.any(mask):
            labeled, num_features = label(mask, structure=structure)
            if num_features > 1:
                component_sizes = np.bincount(labeled.ravel())
                largest_component = component_sizes[1:].argmax() + 1
                result[(labeled != largest_component) & (result == class_id)] = 0
    
    return result

def comprehensive_postprocess(prediction):
    """å®Œæ•´çš„å¾Œè™•ç†æµç¨‹"""
    mode = CONFIG.get("postprocess_mode", "moderate")
    logger.info(f"    é–‹å§‹å¾Œè™•ç† (æ¨¡å¼: {mode})...")
    
    if mode == "minimal":
        # æœ€å°å¾Œè™•ç†
        prediction = remove_small_components(
            prediction,
            {
                1: CONFIG["min_size_class1"],
                2: CONFIG["min_size_class2"],
                3: CONFIG["min_size_class3"]
            }
        )
        logger.info("      âœ… ç§»é™¤å°ç¢ç‰‡")
        
        if CONFIG.get("fill_holes", True):
            prediction = fill_holes_all_classes(prediction)
            logger.info("      âœ… å¡«è£œå­”æ´")
    
    elif mode == "moderate":
        # ä¸­ç­‰å¾Œè™•ç†
        prediction = remove_small_components(
            prediction,
            {
                1: CONFIG["min_size_class1"],
                2: CONFIG["min_size_class2"],
                3: CONFIG["min_size_class3"]
            }
        )
        logger.info("      âœ… ç§»é™¤å°ç¢ç‰‡")
        
        prediction = fill_holes_all_classes(prediction)
        logger.info("      âœ… å¡«è£œå­”æ´")
        
        if CONFIG.get("use_morphology", True):
            prediction = morphological_smooth(prediction)
            logger.info("      âœ… å½¢æ…‹å­¸å¹³æ»‘")
        
        # åªå°å¿ƒè‡Ÿä¿ç•™æœ€å¤§é€£é€šåŸŸ
        prediction = keep_largest_component_per_class(prediction, [1])
        logger.info("      âœ… ä¿ç•™å¿ƒè‡Ÿæœ€å¤§é€£é€šåŸŸ")
    
    elif mode == "aggressive":
        # æ¿€é€²å¾Œè™•ç†
        prediction = remove_small_components(
            prediction,
            {
                1: CONFIG["min_size_class1"],
                2: CONFIG["min_size_class2"],
                3: CONFIG["min_size_class3"]
            }
        )
        logger.info("      âœ… ç§»é™¤å°ç¢ç‰‡")
        
        prediction = fill_holes_all_classes(prediction)
        logger.info("      âœ… å¡«è£œå­”æ´")
        
        prediction = morphological_smooth(prediction)
        logger.info("      âœ… å½¢æ…‹å­¸å¹³æ»‘")
        
        # å°å¿ƒè‡Ÿå’Œå¿ƒåŒ…è†œä¿ç•™æœ€å¤§é€£é€šåŸŸ
        prediction = keep_largest_component_per_class(prediction, [1, 2])
        logger.info("      âœ… ä¿ç•™æœ€å¤§é€£é€šåŸŸ")
        
        # å†æ¬¡æ¸…ç†å°ç¢ç‰‡
        prediction = remove_small_components(
            prediction,
            {
                1: CONFIG["min_size_class1"],
                2: CONFIG["min_size_class2"],
                3: CONFIG["min_size_class3"]
            }
        )
        logger.info("      âœ… æœ€çµ‚æ¸…ç†")
    
    return prediction

# ==================== è¼‰å…¥æ¨¡å‹ ====================
logger.info("="*80)
logger.info("è¼‰å…¥ nnU-Net æ¨¡å‹...")
logger.info("="*80)

model = DynUNet(
    spatial_dims=CONFIG["spatial_dims"],
    in_channels=CONFIG["in_channels"],
    out_channels=CONFIG["out_channels"],
    kernel_size=CONFIG["kernels"],
    strides=CONFIG["strides"],
    upsample_kernel_size=CONFIG["strides"][1:],
    norm_name="instance",
    deep_supervision=CONFIG["deep_supervision"],
    deep_supr_num=CONFIG["deep_supr_num"],
    res_block=True,
).to(CONFIG["device"])

checkpoint = torch.load(CONFIG["model_path"], map_location=CONFIG["device"], weights_only=False)
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

total_params = sum(p.numel() for p in model.parameters())

logger.info(f"âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ")
logger.info(f"   æ¨¡å‹åƒæ•¸é‡: {total_params:,}")
if 'val_dice_mean' in checkpoint:
    logger.info(f"   é©—è­‰ Mean Dice: {checkpoint['val_dice_mean']:.4f}")
    logger.info(f"     - Class 1 (Heart): {checkpoint.get('val_dice_class1', 0):.4f}")
    logger.info(f"     - Class 2 (Pericardium): {checkpoint.get('val_dice_class2', 0):.4f}")
    logger.info(f"     - Class 3 (Calcium): {checkpoint.get('val_dice_class3', 0):.4f}")
    logger.info(f"   è¨“ç·´ Epoch: {checkpoint.get('epoch', 'Unknown')}")

# ==================== è³‡æ–™é è™•ç† ====================
# ç§»é™¤é€™éƒ¨åˆ†,æˆ‘å€‘æœƒåœ¨ predict_case ä¸­å®šç¾©

# ==================== å–å¾—æ¸¬è©¦æª”æ¡ˆ ====================
test_files = sorted(glob.glob(os.path.join(CONFIG["test_dir"], "patient*.nii.gz")))
logger.info(f"\næ‰¾åˆ° {len(test_files)} å€‹æ¸¬è©¦æª”æ¡ˆ")
# é©—è­‰æª”æ¡ˆå®Œæ•´æ€§
logger.info("\né©—è­‰æª”æ¡ˆå®Œæ•´æ€§...")
valid_files = []
corrupted_files = []

for test_file in test_files:
    try:
        # å˜—è©¦è®€å–æª”æ¡ˆé ­
        img = nib.load(test_file)
        valid_files.append(test_file)
    except Exception as e:
        patient_id = os.path.basename(test_file)
        logger.warning(f"âš ï¸ æª”æ¡ˆæå£æˆ–ç„¡æ³•è®€å–: {patient_id} - {str(e)}")
        corrupted_files.append(test_file)

logger.info(f"âœ… æœ‰æ•ˆæª”æ¡ˆ: {len(valid_files)}")
if corrupted_files:
    logger.warning(f"âŒ æå£æª”æ¡ˆ: {len(corrupted_files)}")
    for cf in corrupted_files:
        logger.warning(f"   - {os.path.basename(cf)}")

# ä½¿ç”¨æœ‰æ•ˆæª”æ¡ˆé€²è¡Œé æ¸¬
test_files = valid_files

if len(test_files) > 0:
    logger.info(f"æ¸¬è©¦æª”æ¡ˆç¯„åœ: {os.path.basename(test_files[0])} åˆ° {os.path.basename(test_files[-1])}")
else:
    logger.error("æ²’æœ‰æœ‰æ•ˆçš„æ¸¬è©¦æª”æ¡ˆ!")
    exit(1)

# ==================== é æ¸¬å‡½æ•¸ ====================
def predict_case(image_path):
    """å°å–®ä¸€æ¡ˆä¾‹é€²è¡Œé æ¸¬,ç¢ºä¿æ­£ç¢ºå°é½ŠåŸå§‹å½±åƒç©ºé–“"""
    from monai.transforms import (
        LoadImaged, EnsureChannelFirstd, Orientationd, Spacingd,
        ScaleIntensityRangePercentilesd, CropForegroundd, SpatialPadd,
        Invertd
    )
    from monai.data import MetaTensor
    
    # è¼‰å…¥åŸå§‹å½±åƒè³‡è¨Š
    original_img = nib.load(image_path)
    original_shape = original_img.shape
    original_affine = original_img.affine
    original_header = original_img.header
    
    logger.info(f"  åŸå§‹å°ºå¯¸: {original_shape}")
    
    # å»ºç«‹è³‡æ–™å­—å…¸
    data_dict = {"image": image_path}
    
    # å®šç¾©å¯é€†çš„ transforms
    pre_transforms = Compose([
        LoadImaged(keys=["image"]),
        EnsureChannelFirstd(keys=["image"]),
        Orientationd(keys=["image"], axcodes="LPS"),
        Spacingd(keys=["image"], pixdim=CONFIG["target_spacing"], mode="bilinear"),
        ScaleIntensityRangePercentilesd(
            keys=["image"],
            lower=0.5,
            upper=99.5,
            b_min=0.0,
            b_max=1.0,
            clip=True,
        ),
        CropForegroundd(keys=["image"], source_key="image"),
        SpatialPadd(keys=["image"], spatial_size=CONFIG["roi_size"]),
    ])
    
    logger.info("  é è™•ç†å½±åƒ...")
    data_dict = pre_transforms(data_dict)
    image = data_dict["image"]
    
    logger.info(f"  é è™•ç†å¾Œå°ºå¯¸: {image.shape[1:]}")
    
    # é æ¸¬
    logger.info("  åŸ·è¡Œæ¨ç†...")
    image_tensor = image.unsqueeze(0).to(CONFIG["device"])
    
    with torch.no_grad():
        output = sliding_window_inference(
            image_tensor,
            CONFIG["roi_size"],
            sw_batch_size=CONFIG["sw_batch_size"],
            predictor=model,
            overlap=CONFIG["overlap"],
            mode="gaussian"
        )
        
        if isinstance(output, list):
            output = output[0]
    
    # å–å¾—é æ¸¬çµæœ
    prediction = torch.argmax(output, dim=1).squeeze(0).cpu().numpy()
    logger.info(f"  é æ¸¬å°ºå¯¸: {prediction.shape}")
    
    # å¾Œè™•ç†
    if CONFIG["use_postprocess"]:
        prediction = comprehensive_postprocess(prediction)
    
    # å°‡é æ¸¬çµæœè½‰æ›ç‚º MetaTensor ä»¥ä½¿ç”¨ inverse transform
    pred_metatensor = MetaTensor(
        torch.from_numpy(prediction).unsqueeze(0).float(),
        meta=image.meta
    )
    
    # åå‘è½‰æ›
    logger.info("  åå‘è½‰æ›åˆ°åŸå§‹ç©ºé–“...")
    post_transforms = Compose([
        Invertd(
            keys=["pred"],
            transform=pre_transforms,
            orig_keys="image",
            meta_keys="pred_meta_dict",
            orig_meta_keys="image_meta_dict",
            meta_key_postfix="meta_dict",
            nearest_interp=True,
            to_tensor=True,
        )
    ])
    
    # æº–å‚™åå‘è½‰æ›çš„è³‡æ–™
    data_dict["pred"] = pred_metatensor
    data_dict["pred_meta_dict"] = image.meta
    
    try:
        # å˜—è©¦ä½¿ç”¨ Invert
        data_dict = post_transforms(data_dict)
        prediction_original_space = data_dict["pred"].squeeze().numpy().astype(np.uint8)
    except Exception as e:
        logger.warning(f"  Invert å¤±æ•—: {e}, ä½¿ç”¨ç°¡å–® resize")
        # å¦‚æœ invert å¤±æ•—,ä½¿ç”¨ç°¡å–®çš„ resize
        from scipy.ndimage import zoom
        
        zoom_factors = [
            orig / pred 
            for orig, pred in zip(original_shape, prediction.shape)
        ]
        prediction_original_space = zoom(
            prediction, 
            zoom_factors, 
            order=0  # nearest neighbor
        ).astype(np.uint8)
    
    logger.info(f"  æœ€çµ‚å°ºå¯¸: {prediction_original_space.shape}")
    
    # çµ±è¨ˆè³‡è¨Š
    class_counts = {
        0: np.sum(prediction_original_space == 0),
        1: np.sum(prediction_original_space == 1),
        2: np.sum(prediction_original_space == 2),
        3: np.sum(prediction_original_space == 3)
    }
    
    return prediction_original_space, original_affine, original_header, class_counts

# ==================== æ‰¹æ¬¡é æ¸¬ ====================
logger.info("\n" + "="*80)
logger.info("é–‹å§‹é æ¸¬...")
logger.info("="*80)
logger.info(f"è¼¸å‡ºæ ¼å¼: 0=èƒŒæ™¯, 1=å¿ƒè‡Ÿè‚Œè‚‰, 2=ä¸»å‹•è„ˆå¿ƒåŒ…è†œ, 3=éˆ£åŒ–")
logger.info(f"å¾Œè™•ç†: {'âœ… é–‹å•Ÿ' if CONFIG['use_postprocess'] else 'âŒ é—œé–‰'} (æ¨¡å¼: {CONFIG.get('postprocess_mode', 'minimal')})")
logger.info(f"Device: {CONFIG['device']}")
logger.info("="*80)

success_count = 0
failed_cases = []

for test_file in tqdm(test_files, desc="é æ¸¬ä¸­"):
    patient_id = os.path.basename(test_file).replace('.nii.gz', '')
    
    try:
        logger.info(f"\nè™•ç† {patient_id}...")
        
        # æ¸…ç† GPU ç·©å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # é æ¸¬
        prediction, affine, header, class_counts = predict_case(test_file)
        
        # é¡¯ç¤ºçµ±è¨ˆ
        total_pixels = prediction.size
        logger.info(f"  é æ¸¬çµ±è¨ˆ:")
        logger.info(f"    èƒŒæ™¯: {class_counts[0]:,} pixels ({class_counts[0]/total_pixels*100:.2f}%)")
        logger.info(f"    å¿ƒè‡Ÿ: {class_counts[1]:,} pixels ({class_counts[1]/total_pixels*100:.2f}%)")
        logger.info(f"    å¿ƒåŒ…è†œ: {class_counts[2]:,} pixels ({class_counts[2]/total_pixels*100:.2f}%)")
        logger.info(f"    éˆ£åŒ–: {class_counts[3]:,} pixels ({class_counts[3]/total_pixels*100:.2f}%)")
        
        # å„²å­˜é æ¸¬çµæœ
        output_path = os.path.join(CONFIG["output_dir"], f"{patient_id}_predict.nii.gz")
        pred_img = nib.Nifti1Image(prediction.astype(np.uint8), affine, header)
        nib.save(pred_img, output_path)
        
        success_count += 1
        logger.info(f"  âœ… å„²å­˜æˆåŠŸ: {output_path}")
        
    except Exception as e:
        logger.error(f"  âŒ é æ¸¬å¤±æ•— {patient_id}: {str(e)}")
        # åªè¨˜éŒ„ç°¡çŸ­éŒ¯èª¤è¨Šæ¯,é¿å… logging å•é¡Œ
        failed_cases.append(patient_id)

# ==================== ç¸½çµ ====================
logger.info("\n" + "="*80)
logger.info(f"é æ¸¬å®Œæˆ!")
logger.info("="*80)
logger.info(f"æˆåŠŸ: {success_count}/{len(test_files)}")
if failed_cases:
    logger.warning(f"å¤±æ•—æ¡ˆä¾‹: {failed_cases}")
else:
    logger.info("âœ… æ‰€æœ‰æ¡ˆä¾‹é æ¸¬æˆåŠŸ!")

# ==================== å»ºç«‹æäº¤æª”æ¡ˆ ====================
import zipfile

zip_path = os.path.join(os.path.dirname(CONFIG["output_dir"]), "submission_nnunet_all_classes.zip")

logger.info(f"\nå»ºç«‹æäº¤æª”æ¡ˆ...")
with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
    pred_files = sorted(glob.glob(os.path.join(CONFIG["output_dir"], "*_predict.nii.gz")))
    for pred_file in tqdm(pred_files, desc="å£“ç¸®ä¸­"):
        zipf.write(pred_file, os.path.basename(pred_file))

logger.info(f"âœ… æäº¤æª”æ¡ˆå·²å»ºç«‹: {zip_path}")
logger.info(f"ğŸ“¦ åŒ…å« {len(pred_files)} å€‹é æ¸¬æª”æ¡ˆ")

# æª¢æŸ¥æäº¤æª”æ¡ˆ
logger.info(f"\næª¢æŸ¥æäº¤æª”æ¡ˆå…§å®¹...")
with zipfile.ZipFile(zip_path, 'r') as zipf:
    file_list = sorted(zipf.namelist())
    logger.info(f"å£“ç¸®æª”ä¸­æœ‰ {len(file_list)} å€‹æª”æ¡ˆ")
    
    # é¡¯ç¤ºå‰5å€‹å’Œå¾Œ5å€‹æª”æ¡ˆ
    if len(file_list) > 10:
        logger.info(f"å‰5å€‹æª”æ¡ˆ: {file_list[:5]}")
        logger.info(f"å¾Œ5å€‹æª”æ¡ˆ: {file_list[-5:]}")
    else:
        logger.info(f"æ‰€æœ‰æª”æ¡ˆ: {file_list}")
    
    # æª¢æŸ¥æ˜¯å¦æœ‰ç¼ºå°‘çš„æª”æ¡ˆ(å‡è¨­å¾ patient0051 é–‹å§‹)
    # æ ¹æ“šå¯¦éš›æ¸¬è©¦é›†ç¯„åœèª¿æ•´
    first_patient = int(os.path.basename(test_files[0]).replace('patient', '').replace('.nii.gz', ''))
    last_patient = int(os.path.basename(test_files[-1]).replace('patient', '').replace('.nii.gz', ''))
    
    expected_files = [f"patient{i:04d}_predict.nii.gz" for i in range(first_patient, last_patient + 1)]
    missing_in_zip = set(expected_files) - set(file_list)
    
    if missing_in_zip:
        logger.warning(f"âš ï¸ å£“ç¸®æª”ä¸­ç¼ºå°‘ {len(missing_in_zip)} å€‹æª”æ¡ˆ:")
        for mf in sorted(missing_in_zip)[:10]:  # åªé¡¯ç¤ºå‰10å€‹
            logger.warning(f"   - {mf}")
        if len(missing_in_zip) > 10:
            logger.warning(f"   ... é‚„æœ‰ {len(missing_in_zip) - 10} å€‹")
    else:
        logger.info(f"âœ… å£“ç¸®æª”å…§å®¹å®Œæ•´ (patient{first_patient:04d} åˆ° patient{last_patient:04d})")

logger.info(f"\n" + "="*80)
logger.info("âœ¨ å…¨éƒ¨å®Œæˆ!")
logger.info("="*80)
logger.info(f"\næ¨¡å‹ç­–ç•¥:")
logger.info(f"  - nnU-Net (DynUNet): ç›´æ¥é æ¸¬æ‰€æœ‰é¡åˆ¥ (1, 2, 3)")
logger.info(f"  - Deep Supervision: {CONFIG['deep_supervision']}")
logger.info(f"  - èˆ‡è¨“ç·´é…ç½®å®Œå…¨ä¸€è‡´")
logger.info(f"\næ¨ç†è¨­å®š:")
logger.info(f"  - ROI Size: {CONFIG['roi_size']}")
logger.info(f"  - Overlap: {CONFIG['overlap']}")
logger.info(f"  - Mode: Gaussian")
logger.info(f"  - SW Batch Size: {CONFIG['sw_batch_size']}")
logger.info(f"\nå¾Œè™•ç†ç­–ç•¥:")
if CONFIG["use_postprocess"]:
    logger.info(f"  âœ… å¾Œè™•ç†å·²å•Ÿç”¨ (æ¨¡å¼: {CONFIG.get('postprocess_mode', 'minimal')})")
    logger.info(f"    - ç§»é™¤å°ç¢ç‰‡ (Class1â‰¥{CONFIG['min_size_class1']}, Class2â‰¥{CONFIG['min_size_class2']}, Class3â‰¥{CONFIG['min_size_class3']} voxels)")
    if CONFIG.get("fill_holes", True):
        logger.info(f"    - å¡«è£œå­”æ´")
    if CONFIG.get("use_morphology", False):
        logger.info(f"    - å½¢æ…‹å­¸å¹³æ»‘")
else:
    logger.info(f"  âŒ å¾Œè™•ç†æœªå•Ÿç”¨")

logger.info(f"\nè¼¸å‡ºä½ç½®:")
logger.info(f"  - é æ¸¬è³‡æ–™å¤¾: {CONFIG['output_dir']}")
logger.info(f"  - æäº¤å£“ç¸®æª”: {zip_path}")
logger.info("="*80)